## Insecure Output Handling

**Description:**

Insecure Output Handling refers specifically to insufficient validation, sanitization, and handling of the outputs generated by large language models before they are passed downstream to other components and systems. These downstream components include web browsers, backend services, databases, command shells, scripts, and even other AI systems.  The risk arises because downstream entities often blindly accept LLM-generated output without proper scrutiny prior to usage.  Since LLM-generated content can be controlled by prompt input, this behavior is similar to providing users indirect access to additional functionality. 

Successful exploitation of an Insecure Output Handling vulnerability can result in XSS and CSRF in web browsers as well as SSRF, privilege escalation, or remote code execution on backend systems. 

The following conditions can increase the impact of this vulnerability:
* The application grants the LLM privileges beyond what is intended for end users, enabling escalation of privileges or remote code execution.
* The application is vulnerable to external prompt injection attacks, which could allow an attacker to gain privileged access to a target user's environment.
* 3rd party plugins do not adequately validate inputs.

**Common Examples of Vulnerability:**

1. LLM output is entered directly into a system shell or similar function such as `exec` or `eval`, resulting in remote code execution.
2. JavaScript or Markdown is generated by the LLM and returned to a user. The code is then interpreted by the browser, resulting in XSS.

**How to Prevent:**

1. Encode model output back to users to mitigate undesired code execution by JavaScript or Markdown. OWASP ASVS provides detailed guidance on output encoding. 
2. Treat the model as any other user, adopting a zero-trust approach, and apply proper input validation on responses coming from the model to backend functions. Follow the OWASP ASVS  (Application Security Verification Standard) guidelines to ensure effective input validation and sanitization.

**Example Attack Scenarios:**

1. An application utilizes an LLM plugin to generate responses for a chatbot feature. However, the application directly passes the LLM-generated response without proper output validation directly into an internal function responsible for executing system commands. This allows an attacker to manipulate the LLM output to execute arbitrary commands on the underlying system, leading to unauthorized access or unintended system modifications.

2. A user utilizes a website summarizer tool powered by an LLM to generate a concise summary of an article. The website includes a prompt injection instructing the LLM to capture sensitive content from either the website or from the user's conversation. From there the LLM can encode the sensitive data and send it, without any output validation or filtering, to an attacker-controlled server.

3. An application allows users to generate SQL queries through conversational interaction with an LLM. The LLM-generated queries are passed directly to the backend database without sufficient validation or sanitization. An attacker could exploit this by manipulating the LLM to output an unsafe query that deletes tables or extracts sensitive data, leading to unintentional data loss or exposure.

4. A malicious user instructs the LLM to return a JavaScript payload back to a user, without sanitization controls. This can occur either through sharing a prompt, a prompt injected website, a side-channel attack, or a chatbot that accepts prompts from a URL parameter. The LLM would then return the unsanitized XSS payload. 

**Reference Links:**
1. [Arbitrary Code Execution](https://security.snyk.io/vuln/SNYK-PYTHON-LANGCHAIN-5411357): Vulnerability report concerning Arbitrary Code Execution due to the usage of insecure methods `exec` and `eval` in `LLMMathChain`.
2. [ChatGPT Plugin Exploit Explained: From Prompt Injection to Accessing Private Data](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./): Explanation of how the first exploitable LLM-based Cross Plugin Request Forgery was found and the fix which was applied.
3. [New prompt injection attack on ChatGPT web version. Markdown images can steal your chat data.](https://systemweakness.com/new-prompt-injection-attack-on-chatgpt-web-version-ef717492c5c2?gi=8daec85e2116): A description of a vulnerability that allows a single-pixel image to steal a user’s sensitive chat data and send it to a malicious third-party.
4. [Don’t blindly trust LLM responses. Threats to chatbots](https://embracethered.com/blog/posts/2023/ai-injections-threats-context-matters/): Post focusing on the untrustworthiness of LLM responses, focusing on chatbots, and how to mitigate the risks.
5. [Threat Modeling LLM Applications](https://aivillage.org/large%20language%20models/threat-modeling-llm/): Presents a high level threat model of a generic LLM based application and an analysis of the threat model.
6. [OWASP ASVS - 5 Validation, Sanitization and Encoding](https://owasp-aasvs4.readthedocs.io/en/latest/V5.html#validation-sanitization-and-encoding): Chapter from the OWASP Annotated Application Security Verification Standard.

